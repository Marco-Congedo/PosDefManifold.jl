<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Intro to Riemannian Geometry ¬∑ PosDefManifold</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="PosDefManifold logo"/></a><h1>PosDefManifold</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../MainModule/">MainModule (PosDefManifold.jl)</a></li><li><a class="toctext" href="../">PosDefManifold Documentation</a></li><li class="current"><a class="toctext" href>Intro to Riemannian Geometry</a><ul class="internal"><li><a class="toctext" href="#Riemannian-manifolds-1">Riemannian manifolds</a></li><li><a class="toctext" href="#geodesic-1">geodesic</a></li><li><a class="toctext" href="#distance-1">distance</a></li><li><a class="toctext" href="#distance-from-the-origin-1">distance from the origin</a></li><li><a class="toctext" href="#mean-1">mean</a></li><li><a class="toctext" href="#Fr√©chet-mean-1">Fr√©chet mean</a></li><li><a class="toctext" href="#invariances-1">invariances</a></li><li><a class="toctext" href="#metrics-1">metrics</a></li><li><a class="toctext" href="#-1">üéì</a></li></ul></li><li><a class="toctext" href="../linearAlgebra/">linearAlgebra.jl</a></li><li><a class="toctext" href="../riemannianGeometry/">riemannianGeometry.jl</a></li><li><a class="toctext" href="../signalProcessing/">signalProcessing.jl</a></li><li><a class="toctext" href="../statistics/">statistics.jl</a></li><li><a class="toctext" href="../test/">test.jl</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Intro to Riemannian Geometry</a></li></ul><a class="edit-page" href="https://github.com/Marco-Congedo/PosDefManifold.jl/blob/master/docs/src/introToRiemannianGeometry.md"><span class="fa">ÔÇõ</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Intro to Riemannian Geometry</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Intro-to-Riemannian-Geometry-1" href="#Intro-to-Riemannian-Geometry-1">Intro to Riemannian Geometry</a></h1><p>The study of appropriate <em>distance</em> measures for <a href="https://bit.ly/2HJx3pJ">positive definite matrices</a> has recently grown very fast, driven by practical problems in radar data processing, image processing, computer vision, shape analysis, medical imaging (especially diffusion MRI and Brain-Computer Interface), sensor networks, elasticity, mechanics, numerical analysis and machine learning (<em>e.g.</em>, see references in Congedo et <em>al.</em>, 2017a)<a href="#-1">üéì</a>.</p><p>In many applications the observed data can be conveniently summarized by positive definite matrices, which are either <a href="https://bit.ly/2HJx3pJ">symmetric positive definite</a> (SPD: real) or <a href="https://bit.ly/2Y9AfAI">Hermitian Positive Definite</a> (HPD: complex). For example, those may be some form of the data <a href="https://bit.ly/2IJEWcc">covariance matrix</a> in the time, frequency or time-frequency domain, or autocorrelation matrices, kernels, slices of tensors, density matrices, elements of a search space, etc. Positive definite matrices are naturally treated as points on a <em>smooth Riemannian manifold</em> allowing useful operations such as interpolation, smoothing, filtering, approximation, averaging, signal detection and classification. Such operations are the object of the present <em>PosDefManifold</em> library.</p><p>More formally, this <a href="https://julialang.org/">Julia</a> library treats operations on the metric space <span>$($</span><strong>P</strong><span>$, Œ¥^2)$</span> of <em>n„Éªn</em> positive definite matrices endowed with a distance or symmetric divergence <span>$Œ¥($</span><strong>P</strong> <em>x</em> <strong>P</strong><span>$)‚Üí[0, ‚àû]$</span>. Several matrix distances or matrix divergences <span>$Œ¥$</span> are considered. Using some of them, the most important one being the <strong>Fisher metric</strong>, we define a <a href="https://bit.ly/2CrHr1O">Riemannian manifold</a>. In mathematics, this is the subject of <a href="https://bit.ly/2Y9rfLU">Riemannian geometry</a> and <a href="https://bit.ly/2Cqc1J6">information geometry</a>.</p><p>Note that throughout this library the word <em>&#39;metric&#39;</em> is used loosely for referring to the actual Riemannian metric on the tangent space and to the resulting distance or to general symmetric divergence acting on <strong>P</strong>, regardless the fact that we are dealing with a metric in the strict sense and that it induces or not a Riemannian geometry in <strong>P</strong>. This is done for convenience of exposition, since in practice those <em>&#39;metrics&#39;</em> in <em>PosDefManifold</em> may be used interchangeably.</p><h2><a class="nav-anchor" id="Riemannian-manifolds-1" href="#Riemannian-manifolds-1">Riemannian manifolds</a></h2><p>Here are some important definitions:</p><p>A <strong>smooth manifold</strong> in differential geometry is a topological space that is locally similar to the Euclidean space and has a globally defined differential structure.</p><p>The <strong>tangent space</strong> at point <span>$G$</span> is the vector space containing the tangent vectors to all curves on the manifold passing through <span>$G$</span> (Fig. 1).</p><p>A <strong>smooth Riemannian manifold</strong> is equipped with an inner product on the tangent space (a Riemannian metric) defined at each point and varying smoothly from point to point. For manifold <strong>P</strong> the tangent space is the space of symmetric or Hermitian matrices.</p><p>Thus, a Riemannian metric turns the metric space <span>$($</span><strong>P</strong><span>$, Œ¥^2)$</span> into a Riemannian manifold. This is the case, for example, of the <a href="#Fisher-1">Fisher</a> metric, which has a fundamental role in the manifolds of positive definite matrices and of the <a href="#Wasserstein-1">Wasserstein</a> metric, fundamental in optimal transport theory.</p><p><img src="../assets/Fig1.jpg" alt="Figure 1"/> Figure 1. Schematic illustration of the Riemannian manifold of positive definite matrices. Left: geodesic relying points <span>$P$</span> and <span>$Q$</span> passing through its-mid-point (mean) <span>$G$</span> (green curve), tangent space at point <span>$G$</span> with tangent vectors to geodesic from <span>$G$</span> to <span>$P$</span> and from <span>$G$</span> to <span>$Q$</span> (blue arrowed lines) and distance <span>$Œ¥(G, Q)$</span>. Right: the center of mass (also named mean) <span>$G$</span> of points <span>$P1,‚Ä¶,P4$</span> defined as the point minimizing the sum of the four squared distances <span>$Œ¥2(G, P_i)$</span>, for <span>$i={1,‚Ä¶,4}$</span>.</p><h2><a class="nav-anchor" id="geodesic-1" href="#geodesic-1">geodesic</a></h2><p>The key object in the <strong>P</strong> manifold is the <strong>geodesic</strong>, the shortest path joining two points <span>$P$</span> and <span>$Q$</span> on the manifold, analogous to straight lines in the Euclidean space (Fig. 1). The gedesic equation with arclength <span>$0‚â§a‚â§1$</span> is the equation of the points along the path, where with <span>$a=0$</span> we stay at <span>$P$</span> and with <span>$a=1$</span> we move all the way to <span>$Q$</span>. The points along the geodesic in between <span>$P$</span> and <span>$Q$</span> <span>$(0&lt;a&lt;1)$</span> can be understood as <em>weighted means</em> of <span>$P$</span> and <span>$Q$</span>. For example, the geodesic equation according to the Euclidean metric is <span>$(1-a)P + aQ$</span>, which is the traditional way to define weighted means. With the metrics we consider here, geodesics are unique and always exist. Furthermore, as we will see, using the <a href="#Fisher-1">Fisher</a> metric those geodesics extends indefinitely, <em>i.e.</em>, they are definied and always remain positive definite for <span>$-‚àû&lt;a&lt;‚àû$</span>.</p><h2><a class="nav-anchor" id="distance-1" href="#distance-1">distance</a></h2><p>The length of the geodesic (at constant velocity) between two points gives the <strong>distance</strong> <span>$Œ¥(P, Q)$</span>.  The distance is always real, non-negative and equal to zero if and only if <span>$P=Q$</span>.</p><h2><a class="nav-anchor" id="distance-from-the-origin-1" href="#distance-from-the-origin-1">distance from the origin</a></h2><p>In contrast to an Euclidean space, the origin of the <strong>P</strong> manifold endowed with the <a href="#Fisher-1">Fisher</a> metric is not <span>$0_n$</span>, but <span>$I_n$</span>, the identity matrix of dimension <em>n„Éªn</em>. The distance between a point <span>$P$</span> and the origin, <em>i.e.</em>, <span>$Œ¥(P, I)$</span>, is analogous therein to the length of vectors in Euclidean space. This Riemannian manifold is symmetric around <span>$I_n$</span>, <em>i.e.</em>, <span>$Œ¥(P, I)=Œ¥(P^{-1}, I)$</span> and <span>$Œ¥(P, Q)=Œ¥(P^{-1}, Q^{-1})$</span>. This will be made more precise when we talk about <a href="#invariances-1">invariances</a>.</p><h2><a class="nav-anchor" id="mean-1" href="#mean-1">mean</a></h2><p>The mid-point on the geodesic relying <span>$P$</span> and <span>$Q$</span> is named the <strong>mean</strong>. Using the Euclidean metric this is the <em>arithmetic mean</em> of <span>$P$</span> and <span>$Q$</span> and using the inverse Euclidean metric this is their <em>harmonic mean</em>. As we will see, those are straightforward extensions of their scalar counterparts. Using the Fisher metric the mid-point of the geodesic relying <span>$P$</span> and <span>$Q$</span> allows the proper generalization to matrices of the scalars&#39; <em>geometric mean</em>. The other metrics allows other definition of means (see below).</p><h2><a class="nav-anchor" id="Fr√©chet-mean-1" href="#Fr√©chet-mean-1">Fr√©chet mean</a></h2><p>Using <a href="https://bit.ly/2CvhL4f">Fr√©chet&#39;s variational approach</a> we can extend to positive-definite matrices the concept of weighted mean of a set of scalars; as the midpoint <span>$G$</span> on the geodesic relying <span>$P$</span> and <span>$Q$</span> is the minimizer of <span>$\sigma^2(P, G)+\sigma^2(Q, G)$</span>, so the mean <span>$G$</span> of points <span>$P_1, P_2,...,P_k$</span> is the matrix <span>$G$</span> verifying</p><p><span>$\textrm{argmin}_{G}\sum_{i=1}^{k}Œ¥^2(P_i,G).$</span></p><p>Thus, every metric induces a distance (or divergence) function, which, in turn, induces a mean.</p><h2><a class="nav-anchor" id="invariances-1" href="#invariances-1">invariances</a></h2><p>An important characteristic of metrics is that they may induce invariance properties on the distance, which are in turn inherited by the mean.</p><p>Let us denote shortly by <span>$\{P_i\}$</span> the set <span>$\{P_1,...,P_k\}$</span>, where <span>$i=\{1,...,k\}$</span>  and by <span>$G\{P_i\}$</span> the Fr√©chet mean of the set (in this section we drop the weights here for keeping the notation short). The most important invariance properties are:</p><table><tr><th style="text-align: center">invariance</th><th style="text-align: right">effect on distance <span>$Œ¥(P,Q)$</span></th><th style="text-align: right">effect on mean <span>$G\{P_i\}$</span></th></tr><tr><td style="text-align: center">rotation</td><td style="text-align: right"><span>$Œ¥(P,Q)=Œ¥(U^*PU,U^*QU)$</span></td><td style="text-align: right"><span>$G\{U^*P_iU\}=U^*G\{P_i\}U$</span></td></tr><tr><td style="text-align: center">affinity</td><td style="text-align: right"><span>$Œ¥(P,Q)=Œ¥(B^*PB,B^*QB)$</span></td><td style="text-align: right"><span>$G\{B^*P_iB\}=B^*G\{P_i\}B$</span></td></tr><tr><td style="text-align: center">inversion</td><td style="text-align: right"><span>$Œ¥(P,Q)=Œ¥(P^{-1},Q^{-1})$</span></td><td style="text-align: right"><span>$G\{P_i^{-1}\}=G^{-1}\{P_i\}$</span></td></tr></table><p>for any unitary <span>$U$</span> unitary and non-singular <span>$B$</span>.</p><p>The affine invariance implies the rotation invariance and is also named <em>congruence invariance</em>.</p><h2><a class="nav-anchor" id="metrics-1" href="#metrics-1">metrics</a></h2><p>We are interested in distance or divergence functions, the difference between the two being that a divergence does not need to be symmetric nor to satisfy the triangle inequality. Note that in <em>PosDefManifold</em> we consider only distances and symmetric divergences. In fact those are of greater interest in practice. One can find several distances and divergences in the literature and they often turn out to be related to each other, see for example (Chebby and Moakher, 2012; Cichocki et <em>al.</em>, 2015; Sra, 2016)<a href="#-1">üéì</a>. Ten of them are implemented in <strong>PosDefManifold</strong> and two of them are Riemannian metrics (the Fisher and Wasserstein metric as we have said). In this section we give a complete list of the expressions for their induced</p><ul><li><em>distance</em> of a point <span>$P$</span> from the <em>origin</em>,</li><li><em>distance</em> between <em>two points</em> <span>$P$</span> and <span>$Q$</span>,</li><li><em>geodesic</em> relying <span>$P$</span> to <span>$Q$</span> (hence the weighted means of <span>$P$</span> and <span>$Q$</span>)</li><li><em>weighted Fr√©chet mean</em> <span>$G(P,w)$</span> of a set of <span>$k&gt;2$</span> points <span>$\{P_1,...,P_k\}$</span> with associated real non-negative weights <span>$\{w_1,...,w_k\}$</span> summing up to 1.</li></ul><div class="admonition note"><div class="admonition-title">Nota Bene</div><div class="admonition-text"><p>In the following, the weights <span>$\{w_1,...,w_k\}$</span> are always supposed summing up to 1, superscript <span>$*$</span> indicate conjugate transpose (or just transpose if the matrix is real) and if <span>$a$</span> is the arclength of a geodesic, we define for convenience <span>$b=1-a$</span>.</p></div></div><h3><a class="nav-anchor" id="Euclidean-1" href="#Euclidean-1">Euclidean</a></h3><p>This is the classical <em>Euclidean distance</em> leading to the usual <strong>arithmetic mean</strong>. In general this metric is not well adapted to the <strong>P</strong> manifold. It verifies only the rotation invariance, however the mean also verifies the congruence invariance.</p><table><tr><th style="text-align: left">distance¬≤ to <span>$I$</span></th><th style="text-align: left">distance¬≤</th></tr><tr><td style="text-align: left"><span>$‚à•P-I‚à•^2$</span></td><td style="text-align: left"><span>$‚à•P-Q‚à•^2$</span></td></tr></table><table><tr><th style="text-align: left">geodesic</th><th style="text-align: left">Fr√©chet mean</th></tr><tr><td style="text-align: left"><span>$bP + aQ$</span></td><td style="text-align: left"><span>$\sum_{i=1}^{k}w_i P_i$</span></td></tr></table><h3><a class="nav-anchor" id="inverse-Euclidean-1" href="#inverse-Euclidean-1">inverse Euclidean</a></h3><p>This is the classical <em>harmonic distance</em> leading to the <strong>harmonic mean</strong>. It verifies only the rotation invariance, however the mean also verifies the congruence invariance.</p><table><tr><th style="text-align: left">distance¬≤ to <span>$I$</span></th><th style="text-align: left">distance¬≤</th></tr><tr><td style="text-align: left"><span>$‚à•P^{-1}-I‚à•^2$</span></td><td style="text-align: left"><span>$‚à•P^{-1}-Q^{-1}‚à•^2$</span></td></tr></table><table><tr><th style="text-align: left">geodesic</th><th style="text-align: left">Fr√©chet mean</th></tr><tr><td style="text-align: left"><span>$\big(bP^{-1} + aQ^{-1}\big)^{-1}$</span></td><td style="text-align: left"><span>$\big(\sum_{i=1}^{k}w_i P_i^{-1}\big)^{-1}$</span></td></tr></table><h3><a class="nav-anchor" id="Cholesky-Euclidean-1" href="#Cholesky-Euclidean-1">Cholesky Euclidean</a></h3><p>This is a very simple metric that has been tried to improve the Euclidean one. It is rarely used (see for example Dai et <em>al.</em>, 2016)<a href="#-1">üéì</a>. It does not verify any invariance. Let <span>$L_P$</span> be the lower triangular <a href="https://bit.ly/1KFkeCN">Cholesky factor</a> of <span>$P$</span>, then</p><table><tr><th style="text-align: left">distance¬≤ to <span>$I$</span></th><th style="text-align: left">distance¬≤</th></tr><tr><td style="text-align: left"><span>$‚à•L_P-I‚à•^2$</span></td><td style="text-align: left"><span>$‚à• L_P-L_Q ‚à•^2$</span></td></tr></table><table><tr><th style="text-align: left">geodesic</th><th style="text-align: left">Fr√©chet mean</th></tr><tr><td style="text-align: left"><span>$(bL_P+aL_Q)(bL_{P}+aL_{Q})^*$</span></td><td style="text-align: left"><span>$\big(\sum_{i=1}^{k}w_i L_{P_i}\big)\big(\sum_{i=1}^{k}w_i L_{P_i}\big)^*$</span></td></tr></table><h3><a class="nav-anchor" id="log-Euclidean-1" href="#log-Euclidean-1">log Euclidean</a></h3><p>If matrices <span>$\{P_1,...,P_k\}$</span> all pair-wise commute, then this metric coincides with the Fisher metric. See (Arsigny et <em>al.</em>, 2007 ; Bhatia et <em>al.</em>, 2019a)<a href="#-1">üéì</a>. It enjoys the rotation and inversion invariance. The log-Euclidean distance to <span>$I$</span> is the same as per the Fisher metric. This mean has the same determinant as the <a href="#Fisher-1">Fisher</a> mean, and trace equal or superior to the trace of the Fisher mean. A minimum trace log Euclidean mean approximating well the Fisher mean has been proposed in Congedo et <em>al.</em> (2015)<a href="#-1">üéì</a>.</p><table><tr><th style="text-align: left">distance¬≤ to <span>$I$</span></th><th style="text-align: left">distance¬≤</th></tr><tr><td style="text-align: left"><span>$‚à•\textrm{log}(P)‚à•^2$</span></td><td style="text-align: left"><span>$‚à•\textrm{log}(P)-\textrm{log}(Q)‚à•^2$</span></td></tr></table><table><tr><th style="text-align: left">geodesic</th><th style="text-align: left">Fr√©chet mean</th></tr><tr><td style="text-align: left"><span>$\textrm{exp}\big(\textrm{log}P + a\textrm{log}Q\big)$</span></td><td style="text-align: left"><span>$\textrm{exp}\big(\sum_{i=1}^{k}w_i\hspace{1pt}\textrm{log}P_i\big)$</span></td></tr></table><h3><a class="nav-anchor" id="log-Cholesky-1" href="#log-Cholesky-1">log Cholesky</a></h3><p>It is a recently proposed distance in <strong>P</strong>. Like the <a href="#Cholesky-Euclidean-1">Cholesky Euclidean</a> metric here above, it exploits the diffeomorphism between matrices in <strong>P</strong> and their Cholesky factor, such that <span>$L_PL_P^*=P$</span>, thanks to the fact that the Cholesky factor is unique and that the map is smooth (Lin, 2019)<a href="#-1">üéì</a>. The mean has the same determinant as the Fisher and log-Euclidean mean.</p><p>Let <span>$L_X$</span>,<span>$S_X$</span> and <span>$D_X$</span> be the lower triangle, the strictly lower triangle and the diagonal part of <span>$X$</span>, respectively (hence, <span>$S_X+D_X=L_X$</span>), then</p><table><tr><th style="text-align: center">Distance¬≤ to <span>$I$</span></th><th style="text-align: right">Distance¬≤</th></tr><tr><td style="text-align: center"><span>$‚à•S_P-I‚à•^2+‚à•\textrm{log}D_P‚à•^2$</span></td><td style="text-align: right"><span>$‚à•S_P-S_Q‚à•^2+‚à•\textrm{log}D_P-\textrm{log}D_Q‚à•^2$</span></td></tr></table><p><strong>geodesic</strong>: <span>$S_P+a(S_Q-S_P)+D_P\hspace{2pt}\textrm{exp}\big(a\textrm{log}D_Q-a\textrm{log}D_P\big)$</span></p><p><strong>Fr√©chet mean</strong>: <span>$TT^*$</span>, where <span>$T=\sum_{i=1}^{k}w_iS_{P_i}+\sum_{i=1}^{k}w_i\textrm{log}D_{P_i}$</span></p><h3><a class="nav-anchor" id="Fisher-1" href="#Fisher-1">Fisher</a></h3><p>The Fisher metric, also known as <em>affine-invariant</em>, <em>natural</em> and <em>Fisher-Rao</em> metric, among others names, has a paramount importance for the <strong>P</strong> manifold, standing out as the natural choice both from the perspective of differential geometry and information geometry. Endowed with the Fisher metric the manifold <strong>P</strong> is Riemannian, has nonpositive curvature and is symmetric. This metric verifies all three <a href="#invariances-1">invariances</a> we have considered.</p><table><tr><th style="text-align: left">Distance¬≤ to <span>$I$</span></th><th style="text-align: right">Distance¬≤</th></tr><tr><td style="text-align: left"><span>$‚à•\textrm{log}(P)‚à•^2$</span></td><td style="text-align: right"><span>$‚à•\textrm{log}(P^{-1/2}QP^{-1/2})‚à•^2$</span></td></tr></table><table><tr><th style="text-align: left">geodesic</th></tr><tr><td style="text-align: left"><span>$P^{1/2} \big(P^{-1/2} Q P^{-1/2}\big)^a P^{1/2}$</span></td></tr></table><p><strong>Fr√©chet mean</strong>: it does not have a closed-form solution in general. The solution is the unique positive definite matrix <strong>G</strong> satisfying (Bhatia and Holbrook, 2006; Moakher, 2005).<a href="#-1">üéì</a></p><p><span>$\sum_{i=1}^{k}w_i\textrm{log}\big(G^{-1/2} P_i G^{-1/2}\big)=0.$</span></p><p>For estimating it, <em>PosDefManifold</em> implements the well-known gradient descent algorithm, resulting in iterations:</p><p><span>$G ‚ÜêG^{1/2}\textrm{exp}\big(\sum_{i=1}^{k}w_i\textrm{log}(G^{-1/2} P_i G^{-1/2})\big)G^{1/2}.$</span></p><p>Alternatively, and more efficiently, one can ask for an approximate solution invoking the MPM algorithm (Congedo et <em>al.</em>, 2017b)<a href="#-1">üéì</a>, which is also implemented (in order to estimate the geometric mean use function <a href="../riemannianGeometry/#PosDefManifold.powerMean"><code>powerMean</code></a> with parameter <span>$p=0$</span> or with a very small value of <span>$p$</span>).</p><p>This mean is known under many different names (Fisher, Rao, Fisher-Rao, Pusz-Woronowicz, Cartan, Fr√©chet, Karcher, <strong>geometric</strong>....). The ‚Äòcentrality‚Äô of this mean among a wide family of divergence-based means can be appreciated in Fig. 4 of Cichocki et <em>al.</em> (2015)<a href="#-1">üéì</a>.</p><p>The geometric mean <span>$G$</span> of two matrices <span>$P$</span> and <span>$Q$</span> is denoted shortly as <span>$P\textrm{#}Q$</span>. Currently it is an object of intense study because of its interesting mathematical properties. For instance,</p><ul><li>it is the unique solution to Riccati equation <span>$GQ^{-1}G=P$</span></li><li>it is equal to <span>$F^{-*}D_1^{1/2}D_2^{1/2}F^{-1}$</span> for whatever joint diagonalizer <span>$F$</span> of <span>$P$</span> and <span>$Q$</span>, <em>i.e.</em>, for whatever matrix <span>$F$</span> satisfying <span>$F^*PF=D_1$</span> and <span>$F^*QF=D_2$</span>, with <span>$D_1$</span>, <span>$D_1$</span> non-singular diagonal matrices (Congedo et <em>al.</em>, 2015)<a href="#-1">üéì</a>.</li><li>it enjoys all 10 properties of means postulated in the seminal work of Ando et <em>al.</em> (2010)<a href="#-1">üéì</a>.</li></ul><p>When <span>$P$</span> and <span>$Q$</span> commutes, the Fisher mean of two matrices reduces to <span>$P^{1/2}Q^{1/2}$</span>, which indeed in this case is the log-Euclidean mean <span>$\frac{1}{2}\textrm{log}P + \frac{1}{2}\textrm{log}Q$</span>.</p><p>The Fisher geodesic equation is usually denoted <span>$P\textrm{#}_aQ$</span>. Note that <span>$I\textrm{#}_aP=P^a$</span> and <span>$P\textrm{#}_aI=P^{b}$</span>, where <span>$b=1-a$</span>.</p><p>Fisher geodesic equation verifies <span>$P\textrm{#}_aQ=Q\textrm{#}_{b}P$</span> and <span>$(P\textrm{#}_aQ)^{-1}=P^{-1}\textrm{#}_aQ^{-1}$</span>.</p><p>An interesting property of the Fisher metric is that using its geodesic equation we can extrapolate positive matrices, always remaining in <strong>P</strong>. That is, using any real value of <span>$a$</span> :</p><ul><li>with <span>$0 &lt; a &lt; 1$</span> we move toward <span>$Q$</span>		(<strong>attraction</strong>),</li><li>with <span>$a &gt; 1$</span> we move over and beyond <span>$Q$</span>	(<strong>extrapolation</strong>) and</li><li>with <span>$a&lt; 0$</span> we move back away from <span>$Q$</span> 	(<strong>repulsion</strong>).</li></ul><p>Something similar can be done using the <a href="#log-Cholesky-1">log Cholesky</a> metric as well.</p><h3><a class="nav-anchor" id="power-means-1" href="#power-means-1">power means</a></h3><p>The arithmetic, harmonic and geometric mean we have encountered are all members of the 1-parameter family of <em>power means</em> (with parameter <span>$p‚àä[-1, 1]$</span>) introduced by Lim and Palfia (2012)<a href="#-1">üéì</a> to generalize the concept of power means of scalars (also known as H√∂lder means or <a href="https://bit.ly/2Fpjpp0">generalized means</a>). The family of power means <span>$G$</span> with parameter <span>$p$</span> satisfies equation</p><p><span>$G=\sum_{i=1}^{k}w_i\big(G\textrm{#}_pP_i\big)$</span>,</p><p>where<span>$G\textrm{#}_pP_i$</span> is the Fisher geodesic equation we have discussed here above talking about the <a href="#Fisher-1">Fisher</a> metric. In particular:</p><ul><li>with <span>$p=-1$</span> this is the harmonic mean (see the <a href="#inverse-Euclidean-1">inverse Euclidean</a> metric)</li><li>with <span>$p=+1$</span> this is the arithmetic mean (see the <a href="#Euclidean-1">Euclidean</a> metric)</li><li>at the limit of <span>$p$</span> evaluated at zero from both side this is the geometric mean (see the <a href="#Fisher-1">Fisher</a> metric).</li></ul><p>Thus, the family of power means continuously interpolate between the arithmetic and harmonic mean passing through the the geometric mean.</p><p>All power means enjoy the congruence invariance (hence the rotation invariance), but only the geometric mean enjoy also the inversion invariance.</p><p>The power mean with <span>$p=\frac{1}{2}$</span> is the solution of the <a href="#Fr√©chet-mean-1">Fr√©chet mean</a> problem using the following divergence (Bhatia, Gaubert and Jain, 2019)<a href="#-1">üéì</a></p><p><span>$Œ¥^2(P,Q)=\textrm{tr}(P+Q)-2\textrm{tr}P\textrm{#}Q = \textrm{tr}(\textrm{arithm. mean}(P, Q)) ‚Äì \textrm{tr}(\textrm{geom. mean}(P, Q)).$</span></p><h3><a class="nav-anchor" id="generalized-means-1" href="#generalized-means-1">generalized means</a></h3><p>When the matrices in the set all pairwise commute, it has been proved in Lim and Palfia (2012, see Property 1, p. 1502) <a href="#-1">üéì</a> that the <a href="#power-means-1">power means</a> we have just seen reduce to</p><p><span>$\big(\sum_{i=1}^{k}w_iP_i^p\big)^{1/p}$</span>,</p><p>which are the straightforward extension of scalar power means (see <a href="https://bit.ly/2Fpjpp0">generalized means</a>) to matrices. As usual, such straightforward extensions work well in commuting algebra, but not in general. See for example the case of the mean obtained using the <a href="#log-Euclidean-1">log Euclidean</a> metric, which is the straightforward extension to matrices of the scalar geometric mean, but is <em>not</em> the matrix geometric mean, unless the matrices all pairwise commute.</p><p>Both the generalized means and the <a href="#power-means-1">power means</a> have a parameter <span>$p‚àä[-1, 1]$</span>. For the latter, the solution is implemented via the fixed-point MPM algorithm (Congedo et <em>al.</em>, 2017b)<a href="#-1">üéì</a>.</p><h3><a class="nav-anchor" id="modified-Bhattacharyya-mean-1" href="#modified-Bhattacharyya-mean-1">modified Bhattacharyya mean</a></h3><p>If matrices <span>$P_1, P_2,...,P_k$</span> all pair-wise commute, the special case <span>$p=\frac{1}{2}$</span> yields the following instance of <a href="#power-means-1">power means</a> (and of <a href="#generalized-means-1">generalized means</a>):</p><p><span>$\big(\sum_{i=1}^{k}w_iP_i^{1/2}\big)^{1/2}$</span>.</p><p>This mean has been proposed  in a different context by Moakher (2012)<a href="#-1">üéì</a> as a <strong>modified Bhattacharyya mean</strong>, since it is a modification of the Bhattacharyya mean we will encounter next under the name <a href="#logdet-zero-1">logdet zero</a>. It is worth noting that in commuting algebra Moakher‚Äôs mean also corresponds to the mean obtained with the <a href="#Wasserstein-1">Wasserstein</a> metric.</p><h3><a class="nav-anchor" id="logdet-zero-1" href="#logdet-zero-1">logdet zero</a></h3><p>The <em>logdet zero divergence</em>, also known as the square of the <em>Bhattacharyya divergence</em> (Mohaker, 2013)<a href="#-1">üéì</a>, <em>Stein</em> divergence (Harandi et <em>al.</em>, 2016)<a href="#-1">üéì</a>, <em>symmetrized Jensen divergence</em>, the <em>S-divergence</em> (Sra, 2016)<a href="#-1">üéì</a> or the <em>log determinant Œ±-divergence</em> (with Œ±=0, Chebby and Moakher, 2012 <a href="#-1">üéì</a>) is a Jensen-Bregman symmetric divergence enjoying all three <a href="#invariances-1">invariances</a> we have listed.</p><p>Its square root has been shown to be a distance (Sra, 2016)<a href="#-1">üéì</a>. It behaves very similarly to the <a href="#Fisher-1">Fisher</a> metric at short distances (Moakher, 2012; Sra, 2016; Cichocki et <em>al.</em>, 2015; Harandi et <em>al.</em>, 2016) <a href="#-1">üéì</a> and the mean of two matrices in <strong>P</strong> is the same as the Fisher mean  (Harandi et <em>al.</em>, 2016) <a href="#-1">üéì</a>. Thus, it has often been used instead of the Fisher metric because it allows more efficient calculations. In fact, the calculation of this distance requires only three Cholesky decompositions, whereas the computation of the Fisher distance involves extracting generalized eigenvalues.</p><table><tr><th style="text-align: center">distance¬≤ to <span>$I$</span></th><th style="text-align: right">distance¬≤</th></tr><tr><td style="text-align: center"><span>$\textrm{logdet}\frac{1}{2}(P+I)-\frac{1}{2}\textrm{logdet}(P)$</span></td><td style="text-align: right"><span>$\textrm{logdet}\frac{1}{2}(P+Q)-\frac{1}{2}\textrm{logdet}(PQ)$</span></td></tr></table><p><strong>geodesic</strong>: we use the Fr√©chet mean with appropriate weights.</p><p><strong>Fr√©chet mean</strong>: the solution is the unique positive definite matrix <span>$G$</span> satisfying</p><p><span>$\sum_{i=1}^{k}w_i\big(\frac{1}{2}P_i+\frac{1}{2}G\big)^{-1}=G^{-1}$</span>.</p><p>For estimating it <em>PosDefManifold</em> implements the fixed-point iterations (Moakher, 2012, p315)<a href="#-1">üéì</a>:</p><p><span>$G ‚Üê \frac{k}{2}\big(\sum_{i=1}^{k}w_i(P_i+G)^{-1}\big)^{-1}$</span>.</p><p>The logdet zero divergence between <span>$P$</span> and <span>$Q$</span> can also be written as the log-determinant of their arithmetic mean minus the log-determinant of their geometric mean (Moakher, 2012)<a href="#-1">üéì</a>, which thus defines a possible extension to matrices of the useful concept of <a href="https://bit.ly/2UHK8U1">Wiener entropy</a>.</p><h3><a class="nav-anchor" id="logdet-Œ±-1" href="#logdet-Œ±-1">logdet Œ±</a></h3><p>The log determinant <span>$Œ±$</span>-divergence family for <span>$Œ±‚àä[-1‚Ä¶1]$</span> (Chebby and Moakher, 2012)<a href="#-1">üéì</a> allows</p><ul><li>the logdet zero mean for <span>$Œ±=0$</span>,</li><li>the <strong>left Kullback-Leibler mean</strong> for <span>$Œ±=-1$</span> (which is the harmonic mean)</li><li>the <strong>right Kullback-Leibler mean</strong> for <span>$Œ±=1$</span> (which is the arithmetic mean).</li></ul><p>We do not consider the left and right Kullback-Leibler divergences because the related means are trivially the arithmetic and harmonic one (Moakher, 2012). As per the symmetrized Kullback-Leibler divergence, this is known as <a href="#Jeffrey-1">Jeffrey</a> divergence and will be considered next. The log determinant <span>$Œ±$</span>-divergence family of means is not implemented in <strong>PosDefManifold</strong> (besides the special cases <span>$Œ±=(-1, 0, 1)$</span>, since the family of <a href="#power-means-1">power means</a> are implemented.</p><h3><a class="nav-anchor" id="Jeffrey-1" href="#Jeffrey-1">Jeffrey</a></h3><p>This is a Jensen-Bregman symmetric divergence, also known as the symmetrized Kullback-Leibler divergence (see <a href="#logdet-Œ±-1">logdet Œ±</a>) (Faraki et <em>al.</em>, 2015)<a href="#-1">üéì</a>. It enjoyes all three <a href="#invariances-1">invariances</a> we have listed.</p><table><tr><th style="text-align: center">distance¬≤ to <span>$I$</span></th><th style="text-align: right">distance¬≤</th></tr><tr><td style="text-align: center"><span>$\frac{1}{2}\textrm{tr} \big(P+P^{-1}\big)-n$</span></td><td style="text-align: right"><span>$\frac{1}{2}\textrm{tr}(Q^{-1}P+P^{-1}Q)-n$</span></td></tr></table><p><strong>geodesic</strong>: we use the Fr√©chet mean with appropriate weights.</p><p><strong>Fr√©chet mean</strong>: <span>$A^{1/2}\big(A^{-1/2}HA^{-1/2}\big)^{1/2}A^{1/2}$</span>, where <span>$A$</span> is the arithmetic mean (see <a href="#Euclidean-1">Euclidean</a> metric) and <span>$H$</span> is the harmonic mean (see <a href="#inverse-Euclidean-1">inverse Euclidean</a> metric). Thus, the weighted Fr√©chet mean is the geometric mean (see <a href="#Fisher-1">Fisher</a> metric) of the arithmetic and harmonic mean (Moakher, 2012)<a href="#-1">üéì</a>.</p><p>Note that this is the geometric mean only for <span>$k=2$</span>, that is, for scalars, but not in general for matrices, the geometric mean is the geometric mean of the arithmetic mean and harmonic mean (the only metric inducing the geometric mean in general is the Fisher mean).</p><h3><a class="nav-anchor" id="Von-Neumann-1" href="#Von-Neumann-1">Von Neumann</a></h3><p>The Von Neumann divergence is a Jensen-Bregman symmetric divergence (Sra, 2016; Taghia et <em>al.</em>, 2019)<a href="#-1">üéì</a>. It enjoyes only the rotation invariance.</p><table><tr><th style="text-align: left">distance¬≤ to <span>$I$</span></th><th style="text-align: left">distance¬≤</th></tr><tr><td style="text-align: left"><span>$\frac{1}{2}\textrm{tr}(P\textrm{log}P-\textrm{log}P)$</span></td><td style="text-align: left"><span>$\frac{1}{2}\textrm{tr}\big(P(\textrm{log}P-\textrm{log}Q)+Q(\textrm{log}Q-\textrm{log}P)\big)$</span></td></tr></table><p>The <strong>geodesic</strong> and <strong>weighted Fr√©chet mean</strong> for this metric are not available.</p><h3><a class="nav-anchor" id="Wasserstein-1" href="#Wasserstein-1">Wasserstein</a></h3><p>This is an extension to matrices of the <em>Hellinger divergence</em> for vectors and is also known as the <em>Bures divergence</em> in quantum physics, where it is applied on density matrices (unit trace positive-definite matrices). It enjoyes only the rotation invariance. Endowed with the Wasserstein metric the manifold <strong>P</strong> has a Riemannian geometry of nonnegative curvature. See ( Bhatia et <em>al.</em>, 2019a; Bhatia et <em>al.</em>, 2019b)<a href="#-1">üéì</a>.</p><table><tr><th style="text-align: left">distance¬≤ to <span>$I$</span></th><th style="text-align: left">distance¬≤</th></tr><tr><td style="text-align: left"><span>$\textrm{tr}(P+I)-2\textrm{tr}(P^{1/2})$</span></td><td style="text-align: left"><span>$\textrm{tr}(P+Q) -2\textrm{tr}\big(P^{1/2}QP^{1/2}\big)^{1/2}$</span></td></tr></table><table><tr><th style="text-align: left">geodesic</th></tr><tr><td style="text-align: left"><span>$b^2P+a^2Q +ab\big[(PQ)^{1/2} +(QP)^{1/2}\big]$</span></td></tr></table><p>The quantity <span>$\textrm{tr}\big(P^{1/2}QP^{1/2}\big)^{1/2}$</span> is known in quantum physics as the <em>fidelity</em> of <span>$P$</span> and  <span>$Q$</span> when those are density matrices (unit-trace positive definite matrices).</p><p><strong>Fr√©chet mean</strong>: the solution is the unique positive definite matrix <span>$G$</span> satisfying (Agueh and Carlier, 2011) <a href="#-1">üéì</a></p><p><span>$G=\sum_{i=1}^{k}w_i\big( G^{1/2}  P_i G^{1/2}\big)^{1/2}$</span>.</p><p>For estimating it, <strong>PosDefManifold</strong> implements the fixed-point algorithm of √Ålvarez-Esteban et <em>al.</em> (2016)<a href="#-1">üéì</a>, giving iterations:</p><p><span>$G ‚Üê G^{-1/2} \big(\sum_{i=1}^{k} w_i(G^{1/2}P_i G^{1/2})^{1/2}\big)^2 G^{-1/2}$</span></p><p>In the special case when the matrices all pair-wise commute, the <a href="#Wasserstein-1">Wasserstein</a> mean is equal to the instance of <a href="#power-means-1">power means</a> and <a href="#generalized-means-1">generalized means</a> with <span>$p=\frac{1}{2}$</span> (Bhatia, Jain and Lim, 2019b)<a href="#-1">üéì</a>, that is, to the <a href="#modified-Bhattacharyya-mean-1">modified Bhattacharyya mean</a>.</p><p>In the special case <span>$k$</span>=2 and equal weight the mean is <span>$W=\frac{1}{4}\big(P+Q+(PQ) ^{1/2}+(QP)^{1/2}\big)$</span>.</p><h2><a class="nav-anchor" id="-1" href="#-1">üéì</a></h2><p><strong>References</strong></p><p>M. Agueh, G. Carlier (2011) <a href="https://bit.ly/2TQspNS">Barycenters in the Wasserstein space</a>, SIAM J. Mat. Anal. Appl. 43, 904-924.</p><p>P. C. √Ålvarez-Esteban, E. del Barrio, J.A. Cuesta-Albertos, C. Matr√°na (2016) <a href="https://bit.ly/2HxDyMS">A fixed-point approach to barycenters in Wasserstein space</a>, Journal of Mathematical Analysis and Applications, 441(2), 744-762.</p><p>T. Ando, C.-K. Li, R. Mathias (2004) <a href="https://bit.ly/2Fre81o">Geometric means</a>, Linear Algebra and its Applications, 385(1), 305-334.</p><p>V. Arsigny, P. Fillard, X. Pennec, N. Ayache (2007) <a href="https://bit.ly/2U1D33v">Geometric means in a novel vector space structure on symmetric positive-definite matrices</a>, SIAM journal on matrix analysis and applications, 29(1), 328-347.</p><p>A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2012) <a href="https://hal.archives-ouvertes.fr/hal-00681328/document">Multi-class Brain Computer Interface Classification by Riemannian Geometry</a>, IEEE Transactions on Biomedical Engineering, 59(4), 920-928.</p><p>A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2013) <a href="https://hal.archives-ouvertes.fr/hal-00820475/document">Classification of covariance matrices using a Riemannian-based kernel for BCI applications</a>, Neurocomputing, 112, 172-178.</p><p>R. Bhatia (2007) Positive Definite Matrices. Princeton University press.</p><p>R. Bhatia, M. Congedo (2019) <a href="https://hal.archives-ouvertes.fr/hal-02023293/document">Procrustes problems in manifolds of positive definite matrices</a> Linear Algebra and its Applications, 563, 440-445.</p><p>R. Bhatia, S. Gaubert, T. Jain (2019) <a href="https://bit.ly/2Yasx9n">Matrix versions of the Hellinger distance</a>, arXiv:1901.01378.</p><p>R. Bhatia, J. Holbrook (2006) <a href="https://bit.ly/2Oksun6">Riemannian geometry and matrix geometric means</a>, Linear Algebra and its Applications, 413 (2-3), 594-618.</p><p>R. Bhatia, T. Jain (2010) <a href="https://bit.ly/2FgJjew">Approximation problems in the Riemannian metric on positive definite matrices</a>, Ann. Funct. Anal., 5(2), 118-126.</p><p>R. Bhatia, T. Jain,Y. Lim (2019a) <a href="https://bit.ly/2YmdERA">Inequalities for the Wasserstein mean of positive definite matrices</a>, Linear Algebra and its Applications, in press.</p><p>R. Bhatia, T. Jain, Y. Lim (2019b) <a href="https://arxiv.org/pdf/1712.01504.pdf">On the Bures-Wasserstein distance between positive definite matrices</a> Expositiones Mathematicae, in press.</p><p>Z. Chebbi, M. Moakher (2012) <a href="https://bit.ly/2Fh2UuZ">Means of Hermitian positive-definite matrices based on the log-determinant Œ±-divergence function</a>, Linear Algebra and its Applications, 436(7), 1872-1889.</p><p>A. Cichocki, S. Cruces, S-I- Amari (2015) <a href="https://bit.ly/2TMdNz3">Log-Determinant Divergences Revisited: Alpha-Beta and Gamma Log-Det Divergences</a>, Entropy, 17(5), 2988-3034.</p><p>R.R. Coifman, Y. Shkolnisky, F.J. Sigworth, A. Singer (2008) <a href="http://bit.ly/2MFB965">Graph Laplacian Tomography From Unknown Random Projections</a>, IEEE Transactions on Image Processing, 17(10), 1891-1899.</p><p>M. Congedo, B. Afsari, A. Barachant, M Moakher (2015) <a href="https://bit.ly/2HGMxum">Approximate Joint Diagonalization and Geometric Mean of Symmetric Positive Definite Matrices</a>, PLoS ONE 10(4): e0121423.</p><p>M. Congedo, A. Barachant, R. Bhatia R (2017a) <a href="https://bit.ly/2HOk5qN">Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review</a>, Brain-Computer Interfaces, 4(3), 155-174.</p><p>M. Congedo, A. Barachant, E. Kharati Koopaei (2017b) <a href="https://bit.ly/2HKEcGk">Fixed Point Algorithms for Estimating Power Means of Positive Definite Matrices</a>, IEEE Transactions on Signal Processing, 65(9), 2211-2220.</p><p>X. Dai, S. Khamis, Y. Zhang, L.S. Davis (2016) <a href="https://bit.ly/2Tj5zta">Parameterizing region covariance: an efficient way to apply sparse codes on second order statistics</a>, arXiv:1602.02822.</p><p>M. Faraki, M. Harandi, F. Porikli (2015) <a href="https://bit.ly/2TTLEGt">More About VLAD: A Leap from Euclidean to Riemannian Manifolds</a>, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston.</p><p>W. F√∂rstner, B. Moonen (1999) <a href="https://bit.ly/2FpFa9g">A metric for covariance matrices</a>, In Krumm K and Schwarze VS eds. Qho vadis geodesia...?, number 1999.6 in tech. report of the Dep. Of Geodesy and Geoinformatics, p.113‚Äì128, Stuttgart University.</p><p>M.T. Harandi, R. Hartley, B. Lovell, C. Sanderson (2016) <a href="https://bit.ly/2UKiYvG">Sparse coding on symmetric positive definite manifolds using bregman divergences</a>, IEEE transactions on neural networks and learning systems, 27 (6), 1294-1306.</p><p>S. Lafon (2004) <a href="http://bit.ly/2Kg4ZMH">Diffusion maps and geometric harmonics</a>, Ph.D. dissertation, Yale University, New Heaven, CT.</p><p>Y. Lim, M. P√°lfia (2012) <a href="https://core.ac.uk/download/pdf/82248854.pdf">Matrix power means and the Karcher mean</a>, Journal of Functional Analysis, 262(4), 1498-1514.</p><p>Z. Lin (2019) Riemannian Geometry of Symmetric Positive Definite Matrices via Cholesky Decomposition. In press.</p><p>M. Moakher (2005) <a href="https://bit.ly/2OiVWJV">A Differential Geometric Approach to the Geometric Mean of Symmetric Positive-Definite Matrices</a>, SIAM Journal on Matrix Analysis and Applications, 26(3), 735-747.</p><p>M. Moakher (2012) Divergence measures and means of symmetric positive-definite matrices, in D.H Lailaw and A. Vilanova (Eds) &quot;New Developments in the Visualization and Processing of Tensor Fields&quot;, Springer, Berlin.</p><p>X. Pennec, P. Fillard, N. Ayache (2006) <a href="https://hal.inria.fr/inria-00614990/document">A Riemannian Framework for Tensor Computing</a>, International Journal of Computer Vision, 66(1), 41-66.</p><p>P.L.C. Rodrigues, M. Congedo, C Jutten (2018) <a href="https://bit.ly/2uICUE1">Multivariate Time-Series Analysis Via Manifold Learning</a>, in Proc. of the the IEEE Statistical Signal Processing Workshop (SSP 2018), Fribourg-en-Brisgau, Germany.</p><p>S. Sra (2016) <a href="https://bit.ly/2FoKSbh">Positive definite matrices and the S-divergence</a>, Proc. Amer. Math. Soc., 144, 2787-2797.</p><p>J. Taghia, M. B√•nkestad, F. Lindsten, T.B. Sch√∂n (2019) <a href="https://bit.ly/2Fqohv2">Constructing the Matrix Multilayer Perceptron and its Application to the VAE</a>, arXiv:1902.01182v1</p><p>S. Umeyama (1988) <a href="https://bit.ly/2Uofyml">An Eigendecomposition Approach to Weighted Graph Matching Problems</a>, IEEE Trans. Pattern. Anal. Mach. Intell., 10(5), 695-703.</p><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">PosDefManifold Documentation</span></a><a class="next" href="../linearAlgebra/"><span class="direction">Next</span><span class="title">linearAlgebra.jl</span></a></footer></article></body></html>
